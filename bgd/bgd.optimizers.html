<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>bgd.optimizers module &mdash; Beyond Gradient Descent 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="bgd.utils module" href="bgd.utils.html" />
    <link rel="prev" title="bgd.nn module" href="bgd.nn.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Beyond Gradient Descent
          </a>
              <div class="version">
                0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../math/mathematical_framework.html">Mathematical Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../structure.html">Structure</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">bgd</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="bgd.html">bgd package</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="bgd.html#subpackages">Subpackages</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="bgd.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="bgd.batch.html">bgd.batch module</a></li>
<li class="toctree-l4"><a class="reference internal" href="bgd.cost.html">bgd.cost module</a></li>
<li class="toctree-l4"><a class="reference internal" href="bgd.errors.html">bgd.errors module</a></li>
<li class="toctree-l4"><a class="reference internal" href="bgd.initializers.html">bgd.initializers module</a></li>
<li class="toctree-l4"><a class="reference internal" href="bgd.nn.html">bgd.nn module</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">bgd.optimizers module</a></li>
<li class="toctree-l4"><a class="reference internal" href="bgd.utils.html">bgd.utils module</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Beyond Gradient Descent</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">bgd</a></li>
          <li class="breadcrumb-item"><a href="bgd.html">bgd package</a></li>
      <li class="breadcrumb-item active">bgd.optimizers module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/bgd/bgd.optimizers.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-bgd.optimizers">
<span id="bgd-optimizers-module"></span><h1>bgd.optimizers module<a class="headerlink" href="#module-bgd.optimizers" title="Permalink to this headline"></a></h1>
<p>This module contains all the optimizers that are implemented.
Any optimizer implemented needs to inherit from
<a class="reference internal" href="#bgd.optimizers.Optimizer" title="bgd.optimizers.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bgd.optimizers.Optimizer</span></code></a> and to implement its abstract
method (<code class="xref py py-obj docutils literal notranslate"><span class="pre">update</span></code>).</p>
<dl class="py class">
<dt class="sig sig-object py" id="bgd.optimizers.AdamOptimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bgd.optimizers.</span></span><span class="sig-name descname"><span class="pre">AdamOptimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bgd/optimizers.html#AdamOptimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bgd.optimizers.AdamOptimizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#bgd.optimizers.Optimizer" title="bgd.optimizers.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bgd.optimizers.Optimizer</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional) – Constant steplength.</p></li>
<li><p><strong>beta_1</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional) – Exponential decay rate of the moving average of the gradient.</p></li>
<li><p><strong>beta_2</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional) – Exponential decay rate of the moving average of th.</p></li>
<li><p><strong>epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional) – Constant for numeric stability.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.AdamOptimizer.step">
<span class="sig-name descname"><span class="pre">step</span></span><a class="headerlink" href="#bgd.optimizers.AdamOptimizer.step" title="Permalink to this definition"></a></dt>
<dd><p>Current iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.AdamOptimizer.moment_1">
<span class="sig-name descname"><span class="pre">moment_1</span></span><a class="headerlink" href="#bgd.optimizers.AdamOptimizer.moment_1" title="Permalink to this definition"></a></dt>
<dd><p>Last 1st moment vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.AdamOptimizer.moment_2">
<span class="sig-name descname"><span class="pre">moment_2</span></span><a class="headerlink" href="#bgd.optimizers.AdamOptimizer.moment_2" title="Permalink to this definition"></a></dt>
<dd><p>Last 2nd moment vector.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<p class="rubric">References</p>
<dl class="simple">
<dt>ADAM: A Method For Stochastic Optimization</dt><dd><p>Diederik P. Kingma and Jimmy Lei Ba
<a class="reference external" href="https://arxiv.org/pdf/1412.6980.pdf">https://arxiv.org/pdf/1412.6980.pdf</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bgd.optimizers.LBFGS">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bgd.optimizers.</span></span><span class="sig-name descname"><span class="pre">LBFGS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">m=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon=0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_order_optimizer=&lt;bgd.optimizers.AdamOptimizer</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bgd/optimizers.html#LBFGS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bgd.optimizers.LBFGS" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#bgd.optimizers.Optimizer" title="bgd.optimizers.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bgd.optimizers.Optimizer</span></code></a></p>
<p>Quasi-newtonian optimizer with limited memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>m</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">int</span></code>, optional) – Memory size.</p></li>
<li><p><strong>epsilon</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional) – Constant for numeric stability.</p></li>
<li><p><strong>first_order_optimizer</strong> (<a class="reference internal" href="#bgd.optimizers.Optimizer" title="bgd.optimizers.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a>) – First order optimizer used to approximate the Hessian product.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.LBFGS.k">
<span class="sig-name descname"><span class="pre">k</span></span><a class="headerlink" href="#bgd.optimizers.LBFGS.k" title="Permalink to this definition"></a></dt>
<dd><p>Current iteration of L-BFGS.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.LBFGS.previous_grad">
<span class="sig-name descname"><span class="pre">previous_grad</span></span><a class="headerlink" href="#bgd.optimizers.LBFGS.previous_grad" title="Permalink to this definition"></a></dt>
<dd><p>Gradient vector at iteration k-1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.LBFGS.y">
<span class="sig-name descname"><span class="pre">y</span></span><a class="headerlink" href="#bgd.optimizers.LBFGS.y" title="Permalink to this definition"></a></dt>
<dd><p>List of m last gradient differences.
y_t = grad_{t+1} - grad_t</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.LBFGS.s">
<span class="sig-name descname"><span class="pre">s</span></span><a class="headerlink" href="#bgd.optimizers.LBFGS.s" title="Permalink to this definition"></a></dt>
<dd><p>List of m last update vectors.
s_t = H * grad * steplength, where H is the
Hessian matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.LBFGS.alpha">
<span class="sig-name descname"><span class="pre">alpha</span></span><a class="headerlink" href="#bgd.optimizers.LBFGS.alpha" title="Permalink to this definition"></a></dt>
<dd><p>List of m last alpha coefficients
alpha_i = rho_i * s_i.T * grad,
where rho_i = 1. / (s_i.T * y_i).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">References</p>
<dl class="simple">
<dt>Updating Quasi-Newton Matrices with Limited Storage</dt><dd><p>Nocedal, J. (1980)
Mathematics of Computation. 35 (151): 773–782</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bgd.optimizers.MomentumOptimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bgd.optimizers.</span></span><span class="sig-name descname"><span class="pre">MomentumOptimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bgd/optimizers.html#MomentumOptimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bgd.optimizers.MomentumOptimizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#bgd.optimizers.Optimizer" title="bgd.optimizers.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">bgd.optimizers.Optimizer</span></code></a></p>
<p>Simple first order optimizer with momentum support.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional) – Constant steplength.</p></li>
<li><p><strong>momentum</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code>, optional) – Persistence of previous gradient vectors. Old vectors
are re-used to compute the new search direction,
with respect to the momentum value.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.MomentumOptimizer.previous_grad">
<span class="sig-name descname"><span class="pre">previous_grad</span></span><a class="headerlink" href="#bgd.optimizers.MomentumOptimizer.previous_grad" title="Permalink to this definition"></a></dt>
<dd><p>Gradient vector at previous iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">np.ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bgd.optimizers.Optimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bgd.optimizers.</span></span><span class="sig-name descname"><span class="pre">Optimizer</span></span><a class="reference internal" href="../_modules/bgd/optimizers.html#Optimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bgd.optimizers.Optimizer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for first order and second order optimizers.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="bgd.optimizers.Optimizer.gradient_fragments">
<span class="sig-name descname"><span class="pre">gradient_fragments</span></span><a class="headerlink" href="#bgd.optimizers.Optimizer.gradient_fragments" title="Permalink to this definition"></a></dt>
<dd><p>List of tuples of NumPy arrays,
where the number of tuples is equal to the number of
learnable layers in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bgd.optimizers.Optimizer.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">F</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/bgd/optimizers.html#Optimizer.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bgd.optimizers.Optimizer.update" title="Permalink to this definition"></a></dt>
<dd><p>Computes best move in the parameter space at
current iteration using optimization techniques.
All gradient fragments added to gradient_fragments
are flattened and concatenated to get the batch
gradient vector of the whole network.
The optimized delta vector is then split into
several fragments of original shapes. Finally,
those delta fragments are used to update the parameters
of each layer, individually.</p>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="bgd.nn.html" class="btn btn-neutral float-left" title="bgd.nn module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="bgd.utils.html" class="btn btn-neutral float-right" title="bgd.utils module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, Antoine Passemiers and Robin Petit.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>